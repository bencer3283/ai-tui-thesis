@book{empire,
  author = {Hao, Karen},
  publisher = {Penguin Press},
  title = {Empire of AI},
  year = {2025},
}

@inproceedings{parrots,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610-623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@book{reboot,
  author = {Marcus, Gary and Davis, Ernest},
  publisher = {Vintage},
  title = {Rebooting AI},
  year = {2020}
}

@book{atlas,
  author = {Crawford, Kate},
  publisher = {Yale University Press},
  title = {Atlas of AI},
  year = {2021}
}

@inproceedings{strubell-etal-2019-energy,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1355/",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
    abstract = "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice."
}

@online{maori,
    author = {Lee, Angie},
    title = {MƒÅori Speech AI Model Helps Preserve and Promote New Zealand Indigenous Language},
    year = {2024},
    url = {https://web.archive.org/web/20250825063525/https://blogs.nvidia.com/blog/te-hiku-media-maori-speech-ai/#expand}
}

@inproceedings{scraping,
author = {Bianchi, Federico and Kalluri, Pratyusha and Durmus, Esin and Ladhak, Faisal and Cheng, Myra and Nozza, Debora and Hashimoto, Tatsunori and Jurafsky, Dan and Zou, James and Caliskan, Aylin},
title = {Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594095},
doi = {10.1145/3593013.3594095},
abstract = {Machine learning models that convert user-written text descriptions into images are now widely available online and used by millions of users to generate millions of images a day. We investigate the potential for these models to amplify dangerous and complex stereotypes. We find a broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, we find cases of prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, prompting for occupations resulting in amplification of racial and gender disparities, and prompting for objects resulting in reification of American norms. Stereotypes are present regardless of whether prompts explicitly mention identity and demographic language or avoid such language. Moreover, stereotypes persist despite mitigation strategies; neither user attempts to counter stereotypes by requesting images with specific counter-stereotypes nor institutional attempts to add system ‚Äúguardrails‚Äù have prevented the perpetuation of stereotypes. Our analysis justifies concerns regarding the impacts of today‚Äôs models, presenting striking exemplars, and connecting these findings with deep insights into harms drawn from social scientific and humanist disciplines. This work contributes to the effort to shed light on the uniquely complex biases in language-vision models and demonstrates the ways that the mass deployment of text-to-image generation models results in mass dissemination of stereotypes and resulting harms.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1493-1504},
numpages = {12},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@article{Luccioni_Gamazaychikov_Hooker_Pierrard_Strubell_Jernite_Wu_2024b, 
    title={Light bulbs have energy ratings - so why can‚Äôt Ai Chatbots?}, 
    url={https://www.nature.com/articles/d41586-024-02680-3}, 
    journal={Nature News}, 
    publisher={Nature Publishing Group}, 
    author={Luccioni, Sasha and Gamazaychikov, Boris and Hooker, Sara and Pierrard, R√©gis and Strubell, Emma and Jernite, Yacine and Wu, Carole-Jean}, 
    year={2024}, 
    month={Aug},
    doi = {10.1038/d41586-024-02680-3},
}

@online{sama,
    url= {https://web.archive.org/web/20250829145028/https://blog.samaltman.com/reflections},
    title = {Reflections},
    author = {Altman, Sam},
    year = {2025}
}

@online{anthropic,
    url = {https://web.archive.org/web/20250903041039/https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation},
    title = {Anthropic raises 13B Series F at 183B post-money valuation},
    year = {2025},
    author = {Anthropic}
}

@misc{scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@Article{Hofmann2024,
author={Hofmann, Valentin
and Kalluri, Pratyusha Ria
and Jurafsky, Dan
and King, Sharese},
title={AI generates covertly racist decisions about people based on their dialect},
journal={Nature},
year={2024},
month={Sep},
day={01},
volume={633},
number={8028},
pages={147-154},
abstract={Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4--7. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil¬†rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models' overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology.},
issn={1476-4687},
doi={10.1038/s41586-024-07856-5},
url={https://doi.org/10.1038/s41586-024-07856-5}
}

@misc{bommasani2022opportunitiesrisksfoundationmodels,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher R√© and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram√®r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2108.07258}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

@techreport{NBERw32487,
 title = "The Simple Macroeconomics of AI",
 author = "Acemoglu, Daron",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "32487",
 year = "2024",
 month = "May",
 doi = {10.3386/w32487},
 URL = "http://www.nber.org/papers/w32487",
 abstract = {This paper evaluates claims about large macroeconomic implications of new advances in AI. It starts from a task-based model of AI's effects, working through automation and task complementarities. So long as AI's microeconomic effects are driven by cost savings/productivity improvements at the task level, its macroeconomic consequences will be given by a version of Hulten's theorem: GDP and aggregate productivity gains can be estimated by what fraction of tasks are impacted and average task-level cost savings. Using existing estimates on exposure to AI and productivity improvements at the task level, these macroeconomic effects appear nontrivial but modest -- no more than a 0.66\% increase in total factor productivity (TFP) over 10 years. The paper then argues that even these estimates could be exaggerated, because early evidence is from easy-to-learn tasks, whereas some of the future effects will come from hard-to-learn tasks, where there are many context-dependent factors affecting decision-making and no objective outcome measures from which to learn successful performance. Consequently, predicted TFP gains over the next 10 years are even more modest and are predicted to be less than 0.53\%. I also explore AI's wage and inequality effects. I show theoretically that even when AI improves the productivity of low-skill workers in certain tasks (without creating new tasks for them), this may increase rather than reduce inequality. Empirically, I find that AI advances are unlikely to increase inequality as much as previous automation technologies because their impact is more equally distributed across demographic groups, but there is also no evidence that AI will reduce labor income inequality. Instead, AI is predicted to widen the gap between capital and labor income. Finally, some of the new tasks created by AI may have negative social value (such as design of algorithms for online manipulation), and I discuss how to incorporate the macroeconomic effects of new tasks that may have negative social value.},
}

@inproceedings{10.1145/3351095.3372871,
author = {Abebe, Rediet and Barocas, Solon and Kleinberg, Jon and Levy, Karen and Raghavan, Manish and Robinson, David G.},
title = {Roles for computing in social change},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372871},
doi = {10.1145/3351095.3372871},
abstract = {A recent normative turn in computer science has brought concerns about fairness, bias, and accountability to the core of the field. Yet recent scholarship has warned that much of this technical work treats problematic features of the status quo as fixed, and fails to address deeper patterns of injustice and inequality. While acknowledging these critiques, we posit that computational research has valuable roles to play in addressing social problems --- roles whose value can be recognized even from a perspective that aspires toward fundamental social change. In this paper, we articulate four such roles, through an analysis that considers the opportunities as well as the significant risks inherent in such work. Computing research can serve as a diagnostic, helping us to understand and measure social problems with precision and clarity. As a formalizer, computing shapes how social problems are explicitly defined --- changing how those problems, and possible responses to them, are understood. Computing serves as rebuttal when it illuminates the boundaries of what is possible through technical means. And computing acts as synecdoche when it makes long-standing social problems newly salient in the public eye. We offer these paths forward as modalities that leverage the particular strengths of computational work in the service of social change, without overclaiming computing's capacity to solve social problems on its own.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {252-260},
numpages = {9},
keywords = {discrimination, inequality, social change, societal implications of AI},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@misc{zhou2021frequencybaseddistortionscontextualizedword,
      title={Frequency-based Distortions in Contextualized Word Embeddings}, 
      author={Kaitlyn Zhou and Kawin Ethayarajh and Dan Jurafsky},
      year={2021},
      eprint={2104.08465},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.08465}, 
}

@article{doi:10.1126/science.adp1848,
author = {Rishi Bommasani  and Sayash Kapoor  and Kevin Klyman  and Shayne Longpre  and Ashwin Ramaswami  and Daniel Zhang  and Marietje Schaake  and Daniel E. Ho  and Arvind Narayanan  and Percy Liang },
title = {Considerations for governing open foundation models},
journal = {Science},
volume = {386},
number = {6718},
pages = {151-153},
year = {2024},
doi = {10.1126/science.adp1848},
URL = {https://www.science.org/doi/abs/10.1126/science.adp1848},
eprint = {https://www.science.org/doi/pdf/10.1126/science.adp1848},
abstract = {Different policy proposals may disproportionately affect the innovation ecosystem Foundation models (e.g., GPT-4 and Llama 3.1) are at the epicenter of artificial intelligence (AI), driving technological innovation and billions of dollars in investment. This has sparked widespread demands for regulation. Central to the debate about how to regulate foundation models is the process by which foundation models are released (1)‚Äîwhether they are made available only to the model developers, fully open to the public, or somewhere in between. Open foundation models can benefit society by promoting competition, accelerating innovation, and distributing power. However, an emerging concern is whether open foundation models pose distinct risks to society (2). In general, although most policy proposals and regulations do not mention open foundation models by name, they may have an uneven impact on open and closed foundation models. We illustrate tensions that surface‚Äîand that policy-makers should consider‚Äîregarding different policy proposals that may disproportionately damage the innovation ecosystem around open foundation models.}
}

@article{10.1145/3381831,
author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
title = {Green AI},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3381831},
doi = {10.1145/3381831},
abstract = {Creating efficiency in AI research will decrease its carbon footprint and increase its inclusivity as deep learning study should not require the deepest pockets.},
journal = {Commun. ACM},
month = nov,
pages = {54-63},
numpages = {10}
}

@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}

@misc{ramesh2022hierarchicaltextconditionalimagegeneration,
      title={Hierarchical Text-Conditional Image Generation with CLIP Latents}, 
      author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
      year={2022},
      eprint={2204.06125},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.06125}, 
}

@misc{csetbuchanan,
  author = {Ben Buchanan and Andrew Lohn and Micah Musser and Katerina Sedova},
  publisher = {Center for Security and Emerging Technology},
  title = {Truth, Lies, and Automation: How Language Models Could Change Disinformation},
  year = {2021},
  month = {May},
  doi = {10.51593/2021CA003},
}

@online{appelmccrorytamkin2025geoapi,
      author = {Ruth Appel and Peter McCrory and Alex Tamkin and Michael Stern and Miles McCain and Tyler Neylon},
      title = {Anthropic Economic Index report: Uneven geographic and enterprise AI adoption},
      date = {2025-09-15},
      year = {2025},
      url = {www.anthropic.com/research/anthropic-economic-index-september-2025-report},
}

@techreport{NBERw34255,
 title = "How People Use ChatGPT",
 author = "Chatterji, Aaron and Cunningham, Thomas and Deming, David J and Hitzig, Zoe and Ong, Christopher and Shan, Carl Yan and Wadman, Kevin",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "34255",
 year = "2025",
 month = "September",
 doi = {10.3386/w34255},
 URL = "http://www.nber.org/papers/w34255",
}

@misc{kulveit2025gradualdisempowermentsystemicexistential,
      title={Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development}, 
      author={Jan Kulveit and Raymond Douglas and Nora Ammann and Deger Turan and David Krueger and David Duvenaud},
      year={2025},
      eprint={2501.16946},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2501.16946}, 
}

@techreport{NBERw32966,
 title = "The Rapid Adoption of Generative AI",
 author = "Bick, Alexander and Blandin, Adam and Deming, David J",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "32966",
 year = "2024",
 month = "September",
 doi = {10.3386/w32966},
 URL = "http://www.nber.org/papers/w32966",
}

@inproceedings{10.1145/3715275.3732006,
author = {Varoquaux, Gael and Luccioni, Sasha and Whittaker, Meredith},
title = {Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732006},
doi = {10.1145/3715275.3732006},
abstract = {With the growing attention and investment in recent AI approaches such as large language models, the narrative that the larger the AI system the more valuable, powerful and interesting it is is increasingly seen as common sense. But what is this assumption based on, and how are we measuring value, power, and performance? And what are the collateral consequences of this race to ever-increasing scale? Here, we scrutinize the current scaling trends and trade-offs across multiple axes and refute two common assumptions underlying the ``bigger-is-better'' AI paradigm: 1) that performance improvements are driven by increased scale, and 2) that all interesting problems addressed by AI require large-scale models. Rather, we argue that this approach is not only fragile scientifically, but comes with undesirable consequences. First, it is not sustainable, as, despite efficiency improvements, its compute demands increase faster than model performance, leading to unreasonable economic requirements and a disproportionate environmental footprint. Second, it implies focusing on certain problems at the expense of others, leaving aside important applications, e.g. health, education, or the climate. Finally, it exacerbates a concentration of power, which centralizes decision-making in the hands of a few actors while threatening to disempower others in the context of shaping both AI research and its applications throughout society.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {61-75},
numpages = {15},
location = {
},
series = {FAccT '25}
}

@inproceedings{10.1145/3461702.3462563,
author = {Abdalla, Mohamed and Abdalla, Moustafa},
title = {The Grey Hoodie Project: Big Tobacco, Big Tech, and the Threat on Academic Integrity},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462563},
doi = {10.1145/3461702.3462563},
abstract = {As governmental bodies rely on academics' expert advice to shape policy regarding Artificial Intelligence, it is important that these academics not have conflicts of interests that may cloud or bias their judgement. Our work explores how Big Tech can actively distort the academic landscape to suit its needs. By comparing the well-studied actions of another industry (Big Tobacco) to the current actions of Big Tech we see similar strategies employed by both industries. These strategies enable either industry to sway and influence academic and public discourse. We examine the funding of academic research as a tool used by Big Tech to put forward a socially responsible public image, influence events hosted by and decisions made by funded universities, influence the research questions and plans of individual scientists, and discover receptive academics who can be leveraged. We demonstrate how Big Tech can affect academia from the institutional level down to individual researchers. Thus, we believe that it is vital, particularly for universities and other institutions of higher learning, to discuss the appropriateness and the tradeoffs of accepting funding from Big Tech, and what limitations or conditions should be put in place.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {287-297},
numpages = {11},
keywords = {research funding, conflicts of interest},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{abdalla-etal-2023-elephant,
    title = "The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research",
    author = "Abdalla, Mohamed  and
      Wahle, Jan Philip  and
      Ruas, Terry  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Ducel, Fanny  and
      Mohammad, Saif  and
      Fort, Karen",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.734/",
    doi = "10.18653/v1/2023.acl-long.734",
    pages = "13141--13160",
    abstract = "Recent advances in deep learning methods for natural language processing (NLP) have created new business opportunities and made NLP research critical for industry development. As one of the big players in the field of NLP, together with governments and universities, it is important to track the influence of industry on research. In this study, we seek to quantify and characterize industry presence in the NLP community over time. Using a corpus with comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP publication authors, we explore the industry presence in the field since the early 90s. We find that industry presence among NLP authors has been steady before a steep increase over the past five years (180{\%} growth from 2017 to 2022). A few companies account for most of the publications and provide funding to academic researchers through grants and internships. Our study shows that the presence and impact of the industry on natural language processing research are significant and fast-growing. This work calls for increased transparency of industry influence in the field."
}

@misc{thompson2022computationallimitsdeeplearning,
      title={The Computational Limits of Deep Learning}, 
      author={Neil C. Thompson and Kristjan Greenewald and Keeheon Lee and Gabriel F. Manso},
      year={2022},
      eprint={2007.05558},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2007.05558}, 
}

@inproceedings{10.5555/3042573.3042809,
author = {Wagstaff, Kiri L.},
title = {Machine learning that matters},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1851-1856},
numpages = {6},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.1145/3630106.3658542,
author = {Luccioni, Sasha and Jernite, Yacine and Strubell, Emma},
title = {Power Hungry Processing: Watts Driving the Cost of AI Deployment?},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658542},
doi = {10.1145/3630106.3658542},
abstract = {Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of ‚Äúgenerality‚Äù comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and ``general-purpose'' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {85-99},
numpages = {15},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@techreport{iea,
  author = {IEA},
  institution = {IEA},
  title = {Electricity 2024},
  year = {2024},
  url = {https://www.iea.org/reports/electricity-2024},
  location = {Paris},
}

@article{10.1145/3724499,
author = {Li, Pengfei and Yang, Jianyi and Islam, Mohammad A. and Ren, Shaolei},
title = {Making AI Less 'Thirsty'},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3724499},
doi = {10.1145/3724499},
abstract = {Uncovering and addressing the secret water footprint of AI models},
journal = {Commun. ACM},
month = jun,
pages = {54-61},
numpages = {8}
}

@article{doi:10.1073/pnas.1915768117,
author = {Allison Koenecke  and Andrew Nam  and Emily Lake  and Joe Nudell  and Minnie Quartey  and Zion Mengesha  and Connor Toups  and John R. Rickford  and Dan Jurafsky  and Sharad Goel },
title = {Racial disparities in automated speech recognition},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {14},
pages = {7684-7689},
year = {2020},
doi = {10.1073/pnas.1915768117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1915768117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1915768117},
abstract = {Automated speech recognition (ASR) systems are now used in a variety of applications to convert spoken language to text, from virtual assistants, to closed captioning, to hands-free computing. By analyzing a large corpus of sociolinguistic interviews with white and African American speakers, we demonstrate large racial disparities in the performance of five popular commercial ASR systems. Our results point to hurdles faced by African Americans in using increasingly widespread tools driven by speech recognition technology. More generally, our work illustrates the need to audit emerging machine-learning systems to ensure they are broadly inclusive. Automated speech recognition (ASR) systems, which use sophisticated machine-learning algorithms to convert spoken language to text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. Over the last several years, the quality of these systems has dramatically improved, due both to advances in deep learning and to the collection of large-scale datasets used to train the systems. There is concern, however, that these tools do not work equally well for all subgroups of the population. Here, we examine the ability of five state-of-the-art ASR systems‚Äîdeveloped by Amazon, Apple, Google, IBM, and Microsoft‚Äîto transcribe structured interviews conducted with 42 white speakers and 73 black speakers. In total, this corpus spans five US cities and consists of 19.8 h of audio matched on the age and gender of the speaker. We found that all five ASR systems exhibited substantial racial disparities, with an average word error rate (WER) of 0.35 for black speakers compared with 0.19 for white speakers. We trace these disparities to the underlying acoustic models used by the ASR systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus. We conclude by proposing strategies‚Äîsuch as using more diverse training datasets that include African American Vernacular English‚Äîto reduce these performance differences and ensure speech recognition technology is inclusive.}}

@inproceedings{10.1145/3442188.3445928,
author = {Wilson, Christo and Ghosh, Avijit and Jiang, Shan and Mislove, Alan and Baker, Lewis and Szary, Janelle and Trindel, Kelly and Polli, Frida},
title = {Building and Auditing Fair Algorithms: A Case Study in Candidate Screening},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445928},
doi = {10.1145/3442188.3445928},
abstract = {Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of "fairness" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps.In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool.We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {666-677},
numpages = {12},
keywords = {adverse impact testing, algorithm auditing, fairness, four-fifths rule},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{SKARE2024102719,
title = {Artificial intelligence and wealth inequality: A comprehensive empirical exploration of socioeconomic implications},
journal = {Technology in Society},
volume = {79},
pages = {102719},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102719},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24002677},
author = {Marinko Skare and Beata Gavurova and Sanja {Bla≈æeviƒá Buriƒá}},
keywords = {Artificial intelligence, AI capital stock database, Growth, Wealth, Inequality, Technological level, Panel-corrected standard errors},
abstract = {This study introduces a global database on artificial intelligence (AI) capital stock and related AI indicators. Using the data constructed, we investigate the impact of AI and capital stock accumulation on wealth inequality, a dimension not extensively explored in the literature. This study contributes to the growing body of literature on the socioeconomic consequences of AI, with implications for scholars, policymakers, and corporate executives. An innovative database detailing AI capital stock is developed by incorporating data from various sources, including corporate reports, industry databases, and scholarly literature. This novel dataset, focusing on the US, the EU, and Japan from 1995 to 2020, is a critical resource for future investigations. The research methodology is centered on an extended Solow‚ÄìSwan model, conceptualizing AI as a form of capital that can substitute for or complement traditional forms of labor. A panel-corrected standard errors model is used to analyze the data, accounting for potential cross-sectional dependence and heteroscedasticity. Our findings reveal a positive and statistically significant correlation between AI technology adoption, AI capital stock accumulation, and wealth disparity. The analysis further indicates a complex interaction between income and wealth disparities, suggesting a mutually reinforcing cycle. This study fills a significant gap in the existing literature by offering a novel perspective on the distributional impact of AI. Our results underscore the importance of considering the broader socioeconomic implications of AI, extending beyond considerations of immediate productivity and economic growth. This study offers valuable insights for policy formulation and business decision making, emphasizing the necessity of a comprehensive understanding of the influence of AI on wealth distribution.}
}

@misc{mikolov2013efficientestimationwordrepresentations,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@book{societyofmind,
  author = {Minsky, Marvin},
  publisher = {Simon \& Schuster},
  title = {The Society of Mind},
  year = {1988}
}

@article{LENAT1984269,
title = {Why am and eurisko appear to work},
journal = {Artificial Intelligence},
volume = {23},
number = {3},
pages = {269-294},
year = {1984},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(84)90016-X},
url = {https://www.sciencedirect.com/science/article/pii/000437028490016X},
author = {Douglas B. Lenat and John Seely Brown},
abstract = {The am program was constructed by Lenat in 1975 as an early experiment in getting machines to learn by discovery. In the preceding article in this issue of the AI Journal, Ritchie and Hanna focus on that work as they raise several fundamental questions about the methodology of artificial intelligence research. Part of this paper is a response to the specific points they make. It is seen that the difficulties they cite fall into four categories, the most serious of which are omitted heuristics, and the most common of which are miscommunications. Their considerations, and our post-am work on machines that learn, have clarified why am succeeded in the first place, and why it was so difficult to use the same paradigm to discover new heuristics. Those recent insights spawn questions about ‚Äúwhere the meaning really resides‚Äù in the concepts discovered by am. This in turn leads to an appreciation of the crucial and unique role of representation in theory formation, specifically the benefits of having syntax mirror semantics. Some criticism of the paradigm of this work arises due to the ad hoc nature of many pieces of the work; at the end of this article we examine how this very adhocracy may be a potential source of power in itself.}
}

@misc{bruce2024geniegenerativeinteractiveenvironments,
      title={Genie: Generative Interactive Environments}, 
      author={Jake Bruce and Michael Dennis and Ashley Edwards and Jack Parker-Holder and Yuge Shi and Edward Hughes and Matthew Lai and Aditi Mavalankar and Richie Steigerwald and Chris Apps and Yusuf Aytar and Sarah Bechtle and Feryal Behbahani and Stephanie Chan and Nicolas Heess and Lucy Gonzalez and Simon Osindero and Sherjil Ozair and Scott Reed and Jingwei Zhang and Konrad Zolna and Jeff Clune and Nando de Freitas and Satinder Singh and Tim Rockt√§schel},
      year={2024},
      eprint={2402.15391},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.15391}, 
}

@article{Liang2022,
author={Liang, Weixin
and Tadesse, Girmaw Abebe
and Ho, Daniel
and Fei-Fei, L.
and Zaharia, Matei
and Zhang, Ce
and Zou, James},
title={Advances, challenges and opportunities in creating data for trustworthy AI},
journal={Nature Machine Intelligence},
year={2022},
month={Aug},
day={01},
volume={4},
number={8},
pages={669-677},
issn={2522-5839},
doi={10.1038/s42256-022-00516-1},
url={https://doi.org/10.1038/s42256-022-00516-1}
}

@misc{gupta2022maskvitmaskedvisualpretraining,
      title={MaskViT: Masked Visual Pre-Training for Video Prediction}, 
      author={Agrim Gupta and Stephen Tian and Yunzhi Zhang and Jiajun Wu and Roberto Mart√≠n-Mart√≠n and Li Fei-Fei},
      year={2022},
      eprint={2206.11894},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.11894}, 
}

@inproceedings{10.1145/3411764.3445518, author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M}, title = {‚ÄúEveryone wants to do the model work, not the data work‚Äù: Data Cascades in High-Stakes AI}, year = {2021}, isbn = {9781450380966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3411764.3445518}, doi = {10.1145/3411764.3445518}, articleno = {39}, numpages = {15}, keywords = {raters, high-stakes AI, developers, data quality, data politics, data collectors, data cascades, application-domain experts, Uganda, USA, Nigeria, ML, Kenya, India, Ghana, Data, AI}, location = {Yokohama, Japan}, series = {CHI '21} }

@book{behaviorism,
  author = {Watson, J.B.},
  publisher = {W.W. Norton Company, Inc.},
  location = {New York},
  title = {Psychological Care of Infant and Child},
  year = {1928}
}

@book{noamchomskylinguistic,
  author = {Chomsky, Noam},
  publisher = {Mouton \& Co.},
  title = {Syntactic Structures},
  year = {1957}
}

@book{noamchomsky,
  author = {Lyons, John},
  publisher = {Penguin Book},
  location = {New York},
  title = {Noam Chomsky},
  year = {1978}
}

@book{bostromdeeputopia,
  author = {Bostrom, Nick},
  publisher = {Ideapress Publishing},
  location = {Washington, DC},
  title = {Deep Utopia},
  year = {2024}
}

@article{597748ff-2020-3c20-9c06-e358280e09a2,
 ISSN = {00978507, 15350665},
 URL = {http://www.jstor.org/stable/411334},
 author = {Noam Chomsky},
 journal = {Language},
 number = {1},
 pages = {26--58},
 publisher = {Linguistic Society of America},
 reviewed-author = {B. F. Skinner},
 urldate = {2025-11-26},
 volume = {35},
 year = {1959}
}

@article{10.1093/cjres/rsz022,
    author = {Acemoglu, Daron and Restrepo, Pascual},
    title = {The wrong kind of AI? Artificial intelligence and the future of labour demand},
    journal = {Cambridge Journal of Regions, Economy and Society},
    volume = {13},
    number = {1},
    pages = {25-35},
    year = {2019},
    month = {12},issn = {1752-1378},
    doi = {10.1093/cjres/rsz022},
    url = {https://doi.org/10.1093/cjres/rsz022},
    eprint = {https://academic.oup.com/cjres/article-pdf/13/1/25/33213534/rsz022.pdf},
}

@article{Vinuesa2020,
author={Vinuesa, Ricardo
and Azizpour, Hossein
and Leite, Iolanda
and Balaam, Madeline
and Dignum, Virginia
and Domisch, Sami
and Fell{\"a}nder, Anna
and Langhans, Simone Daniela
and Tegmark, Max
and Fuso Nerini, Francesco},
title={The role of artificial intelligence in achieving the Sustainable Development Goals},
journal={Nature Communications},
year={2020},
month={Jan},
day={13},
volume={11},
number={1},
pages={233},
issn={2041-1723},
doi={10.1038/s41467-019-14108-y},
url={https://doi.org/10.1038/s41467-019-14108-y}
}

@article{doi:10.1086/737233,
author = {Ide, Enrique and Talam\`{a}s, Eduard},
title = {Artificial Intelligence in the Knowledge Economy},
journal = {Journal of Political Economy},
volume = {0},
number = {0},
pages = {000-000},
year = {0},
doi = {10.1086/737233},
URL = {https://doi.org/10.1086/737233},
eprint = {https://doi.org/10.1086/737233}
}

@article{https://doi.org/10.3982/ECTA19417,
author = {Moll, Benjamin and Rachel, Lukasz and Restrepo, Pascual},
title = {Uneven Growth: Automation's Impact on Income and Wealth Inequality},
journal = {Econometrica},
volume = {90},
number = {6},
pages = {2645-2683},
keywords = {Inequality, wealth, capital, returns, wages, labor share, technology, automation},
doi = {https://doi.org/10.3982/ECTA19417},
url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA19417},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA19417},
year = {2022}
}

@article{https://doi.org/10.3982/ECTA19815,
author = {Acemoglu, Daron and Restrepo, Pascual},
title = {Tasks, Automation, and the Rise in U.S. Wage Inequality},
journal = {Econometrica},
volume = {90},
number = {5},
pages = {1973-2016},
keywords = {Tasks, automation, productivity, technology, inequality, wages},
doi = {https://doi.org/10.3982/ECTA19815},
url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA19815},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA19815},
year = {2022}
}

@article{690378712,
  title = "When Product Markets Become Collective Traps: The Case of Social Media",
  journal = "American Economic Review",
  volume = "115",
  number = "12",
  pages = "4105-4136",
  year = "2025",
  issn = "0002-8282",
  doi = "10.1257/aer.20231468",
  url = "https://libkey.io/10.1257/aer.20231468",
  author = "Bursztyn, Leonardo and Handel, Benjamin and Jim√©nez-Dur√°n, Rafael and Roth, Christopher"
}

@article{ssrn4874061,
  author = {Berger, Philip G. and Cai, Wei and Qiu, Lin and Shen, Cindy Xinyi},
  title = {Employer and Employee Responses to Generative AI: Early Evidence },
  year = "2024",
  month = {February},
  doi = {http://dx.doi.org/10.2139/ssrn.4874061},
}
