\documentclass[12pt]{article}
\usepackage[letterpaper, total={16cm, 22cm}]{geometry}
\usepackage[stable]{footmisc}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{array}
\usepackage{longtable}
\usepackage{vcell}
\setlength{\columnsep}{1cm}
\PassOptionsToPackage{hyphens}{url}\usepackage[pdfborder={0 0 0.5 [2 2]}]{hyperref}
% \usepackage{fancyhdr}
% \setlength{\headheight}{15pt}
%\pagestyle{fancy}
% \fancyhead[L]{Hidden Cost of Technology}
% \fancyhead[R]{Ben Cheng}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{multirow}
\graphicspath{ {./images/} }
\usepackage[backend=biber, style=authoryear, sorting=none]{biblatex}
\addbibresource{ref.bib}
\usepackage{mathspec}   %加這個就可以設定字體
%\usepackage{xeCJK}       %讓中英文字體分開設置
%\setCJKmainfont{Noto Serif CJK TC} %設定中文為系統上的字型
%\newCJKfontfamily[chineseSans]\CJKsans{Noto Sans CJK TC}
\setmainfont{IBM Plex Serif}
\setsansfont{Space Grotesk}
\setmonofont{IBM Plex Mono}
%\renewcommand{\familydefault}{\sfdefault}
\XeTeXlinebreaklocale "zh"             %這兩行一定要加，中文才能自動換行
\XeTeXlinebreakskip = 0pt plus 1pt     %這兩行一定要加，中文才能自動換行
\renewcommand{\baselinestretch}{1.25}
% \renewcommand{\figurename}{圖}
% \renewcommand{\tablename}{表}
% \renewcommand{\abstractname}{摘要}
% \renewcommand{\contentsname}{目錄}
% \renewcommand{\listtablename}{表格目錄}
%\renewcommand*{\bibfont}{\footnotesize}
\titleformat*{\section}{\Large \bfseries \sffamily}
\titleformat*{\subsection}{\large \bfseries \sffamily}
\titleformat*{\subsubsection}{\bfseries \sffamily}
\setcounter{tocdepth}{2}
\setcounter{biburlucpenalty}{9900}
\setcounter{biburllcpenalty}{9900}
\setcounter{biburlnumpenalty}{9900}

\title{\sffamily{\small MID Working Thesis} \\ \bfseries Pitfalls and alternatives of foundation models}
\author{Ben Po-Sheng Cheng\thanks{Rhode Island School of Design. Contact: pcheng01@risd.edu}}
\date{\today}
\begin{document}
\pagenumbering{roman}
\maketitle
\begin{abstract}
    Ever since OpenAI unveiled ChatGPT to the public, the idea of Artificial General Intelligence (AGI) has stormed the tech industry with its promise to be ``the most impactful technology in human history''. (\cite{sama}) This is part of a larger paradigm shift of AI development towards general capability Foundation Models that consumes humongous resources. The conversational chatbot interface of ChatGPT has become a norm of how people expect to interact with AI. However, the paradigm of foundation model is dangerous. Enormous amount of energy, labor and capital have been poured into the development of foundation model despite its several overlooked issues, like reinforced bias and inequalities, unhealthy distortion of the science community and the environmental cost of datacenters. 

    % The goal of this work is to challenge this AI paradigm by resurfacing its messy logistics and revealing other potential alternatives through devices that embed this information into its tangible user interface. I wish to spark discussions on whether we should be so devoted to this path and provide a pointer towards other possible trajectories for AI's future that are more sustainable, ethical, and humane. 
    The goal of this work is to challenge this AI paradigm by painting an alternative picture of what AI can be when it is on the opposite side of large, cloud-based, omni-capable and corporate-owned. The body of work is a designed electronics device and its tangible user interface. With them, I wish to provoke the conversation around new implications in terms of societal human-AI relationship.
\end{abstract}
\newpage

\tableofcontents
\newpage
\pagenumbering{arabic}
\section{Introduction}
In recent years, AI has definitely become the technology in spotlight. AI startups are raising significant money every year, CEOs feel compelled to implement AI in their companies, while artists and news outlets are filing lawsuits against data infringement and the fear of being replaced by AI among workers are once again soaring. People seem to have mixed and strong feelings (perhaps too strong) about AI, but a proper conversation about what kind of AI we actually want is missing. The narrative around AI is usually extremely polarized -- either a utopian picture or dystopian one. On the utopian side, prominent voice in the industry describe AI as the ``the most impactful technology in human history'', (\cite{sama}) with the potential to solve cancer and climate change. On the other hand, an intelligence machine with super-huamn capability has always been the one of the greatest fear of humanity as seen in sci-fi movies and novels.

Neither of the narratives helps moving the technology forward to where we want it to be, because they concude flooring either the throttle or the break pedal. In reality, what AI actually can become deserves a much more granular answer, and this work seeks to paint a version of that. To begin with, I want to look at the current dominant paradigm of AI development and in particular, as an attempt to outline the potential improvement and also to establish the fact that AI has problems worth fixing, focusing on the issues and downfalls created by the paradigm. Subsequently, other alternative paradigm for AI development are explored, finally leading to my proposal as a designed tangible user interface.

The ultimate goal of this project is, however, to provoke conversations on the different societal implications of AI other than the two extreme utopian/dyspotian narratives. Through the designed device/interface, I wish people can start to see different ways AI can be built, used, and impact our life.

\subsection{Issues of the current paradigm}
Almost all of the modern large language models (including multi-modal ones and image generation models) are based on an architecture called transformer. (\cite{bommasani2022opportunitiesrisksfoundationmodels}) Published by researchers in Google in 2017, (\cite{vaswani2023attentionneed}) way before ChatGPT became popular in around 2022, it is a type of machine learning models specilizes in understanding and generating natural language. It iteratively predicts the most likely next token (word) given a input paragraph.

The original transformer model does not posses the same level of conversational capabilities as GPT models. What OpenAI did to increase the capabilities of GPT was dramatically increasing the number of parameters inside the mathematical model. The original transformer model has up to 213 million parameters, while GPT-3 is almost 800 times bigger with its 175 billion parameter. (\cite{brown2020languagemodelsfewshotlearners}) This paradigm of making models bigger in order to achieve better performance is famously characterized as OpenAI's Scaling Law. (\cite{scaling}) Behind this paradigm, its seemingly impressive capabilities are made possible by an extremely large amount of computing power and taining data. The training of a state-of-the-art large language model require tens of thousands of computing units running non-stop for weeks. Each computing unit can consume up to 300 watts of power when running at full speed. They also generate a lot of heat, which takes additional energy and water to dissipate. Power consumption of the largest modern datacenters are designed to consume around 150 megawatts, equivalent to nearly 122,000 american households.

These humongous models are also commonly referred to as Foundation Models, because they are large (in terms of number of parameters/weights of neural networks), possess general capability, and act as a foundation or intermediate for downstream AI products. (\cite{bommasani2022opportunitiesrisksfoundationmodels}) Their large scale means they are extremely expensive to develop, and only a handful of resourceful companies are able to afford the computing power, energy, water, and labor required. Their general capability means their behavior is unpredictable, and there is a pressing lack of proper benchmarks to evaluate their performance and externalities. They have a broad impact and implications as they are adapted by all kinds of AI applications; however, their intrinsic biases are also inherited by every downstream product, reinforcing and amplifying existing stereotypes and inequalities.

\subsection{Alternatives}
\subsection{Tangible interface}
A tangible interface is a physical device that allows spectators to imagine how it can be used and also what context it can be situated in. It opens a broader conversation on not just what AI can be but also wider speculations of the societal impact of AI. If the alternative AI paradigm in my proposition is small, local, private and personal, naturally the machine learning model has to live in a personal electronics device, and people inevitably interact with the model in some ways through this device.

Ever since the popularity of ChatGPT skyrocketed, the designed interface for Large Language Models has been centered around various forms of chatbot. This make sense, as these models are built to understand and predit natural langauge. However, chatbot alone is a very inefficient way to interact with these models. For example, in task like coding and image generation, it is very rare that the models can produce the ideal result on the first try, usually it takes several iterations and additional prompts to modify the initial result to perfection. Under the hood, these modifying prompts are appended to the initial prompt, making the input for all subsequent requests longer and longer. Additionally, chatbot as a human computer interface also provides no clue to how the technology actually works, and thus deprive the user from learning best practices and trouble-shooting techniques. This is not the say chatbot is the wrong interface but to argue that more control and configurability should be implemented, either in hardware form or software form.

\subsection*{}
Through my designed tangible interface, I wish to illustrate a different type of AI models that is small, local, private and personal, and to explore the new possibilities of how AI can/should be used and the alternative scoietal implication in terms of human-AI relationship.

\section{Unseen downfalls}

\subsection{Nature resources}
As estimated by \cite{strubell-etal-2019-energy}, the training of a large language model can generate more than 5 times the carbon footprint of the lifetime of a car including fuel. Besides training, running these models is also a energy hungry task. According to \cite{10.1145/3630106.3658542}, the energy required to generate an image using large foundation models is equivalent to one fifth of the energy stored in an iPhone battery. Energy consumption of these datacenters is only predicted to continue rising, surpassing that of the entire of state of Japan (1000 TWh) in 2026. (\cite{iea})

Humongous datacenters not only take electricity to run, they also require water for cooling down the computing units. \cite{10.1145/3724499} estimated that the training of OpenAI's 175 billion parameter model GPT-3 consumed 5.4 million liters of water in Microsoft's datacenters. Depending on where the model is hosted, it also consumes 500ml of water for every 10-50 medium-length responses generated.

\subsection{Labor}
As big techs race to spend tens of billions building out the biggest datacenters to support the foundation model boom, land, communities and nature environments are being exploited. On the outskirts of Santiago, Chile, the municipality of Quilicura hosts various factories and Google's datacenter. Only two decades ago, Quilicura was a rural town home to the rich biodiversity of the Atacama Desert. Now, there is only industrial complexes and the ``Quilicura Urban Forest'', built by Google as a community giveback in 2019. However, there are not many residents still living there as it has become an industrial area. Like most datacenters around the world, Google's datacenter also does not provide any job for the locals except for the temporary construction work -- in 2024, a job posting for mechanical technician for the datacenter requires applicant to submit resume only in English. (\cite{empire})

Generative foundation models like GPT are trained on enormous amounts of datasets, many of which are impossible to filter or censor. As a result, the original models produce sensitive content, and companies like OpenAI hire Silicon Valley middlemen like Sama or Scale to access cheap labor in developing countries to moderate the generated content. It is reported that Sama hired people in Kenya for an average of \$1.46 to \$3.74 to do this work. They are exposed to sexual, violent text content including child sexual abuse, slavery, and bestiality while their employer provides minimum mental health support, which often results in serious mental health conditions. (\cite{empire})

These examples highlight the multifaceted impact of western technological advancements. They exploit resources from the underprivileged and destroy their land, community, and lifestyle.

\subsection{Bias and stereotypes}
Foundation models act as a general intermediate for various AI products; they are often fine-tuned, augmented, or adapted for more specific use cases. Because they are trained with extremely large datasets like the Common Crawl, the bias of the datasets is inherited by the models. For example, there is more content on the internet in English than in any other language. The large amount of online content that is hateful, abusive, and toxic inevitably becomes part of the training material and also directs the model's behavior. The overrepresentation, underrepresentation, and misrepresentation across cultural, geographical, and racial groups are also carried through to foundation models.

These intrinsic biases are inherited by downstream applications that adapt foundation models, creating widespread performance disparities for various use cases. For example, \cite{doi:10.1073/pnas.1915768117} looked into how African American English speakers who cannot access reliable speech recognition technologies are in a disadvantageous position when these technologies are used in job interviews or transcribing courtroom proceedings. According to \cite{Hofmann2024}, models exhibit stereotypes about speakers of African American English. They may be more likely to suggest that speakers of African American English be assigned less-prestigious jobs, be convicted of crimes, and be sentenced to death. \cite{10.1145/3442188.3445928} found that models fail to detect faces of people with darker skin tones.

\subsection{Data ripoff}
On top of that, the training data are often obtained without clear consent. Several news outlets have filed lawsuits against foundation model developers like OpenAI and/or Anthropic for illegally using their publications as training materials. These materials are usually consumed by the model training to update its parameters. They only exist in the generic form of numbers and vectors, erasing the identity and intention of the original authors. Image generation models are trained on images created by artists and illustrators, only to eventually replace them in the workplace.

Actress Scarlett Johansson famously accused of OpenAI creating a ChatGPT voice assistant that sounded almost identical to her even after she declined to license her voice to OpenAI. ``I was shocked, angered and in disbelief that Mr. Altman would pursue a voice that sounded so eerily similar to mine that my closest friends and news outlets could not tell the difference,'' she said. (\cite{scarlett}) In response, OpenAI's CEO Sam Altman stated that the model was trained on the voice of another actor.

\subsection{Economic inequality}\label{econ}
It's easy to describe AI as a groundbreaking technological advancement that benefits all of humanity, solving climate change, cancer, or other troubling issues that pose a significant danger to our species. However, none of these promises seem remotely close to being realized, and the actual economic impact of AI paints a much grimmer picture.

\cite{NBERw32487} estimates the Total Factor Productivity\footnote{In simpler terms, the combined productivity of production input (capital and labor). Traditionally its growth is attributed to technological advancement.} growth generated by AI to be 0.66\% over a ten year period, which is much more modest than people might expect. Moreover, as estimated by \cite{690378712}, emerging technologies like social media may have negative impact on social welfare even if they create economy growth on paper. We've already seen various malicious and manipulative uses enabled or exacerbated by AI, like disinformation. (\cite{csetbuchanan})

According to \cite{10.1093/cjres/rsz022}, when new technologies are able to drive substantial productivity growth, even if a large portion of routine tasks are automated and thus lower-skill workers are replaced, the reorganization of production processes creates new tasks and jobs for human labor. However, if the productivity growth is modest or if the new technology's sole focus is to replace human labor, new tasks may not emerge, and task displacement means low-skilled workers will suffer from job loss. This is what Acemoglu described as ``so-so'' automation, and AI, with its estimated 0.66\% productivity growth, seems dangerously close to falling into this group.

Recent developments in economics suggest that technology, in particular those that enable automation in the production process of goods or services, creates an imbalance and unequal outcome for people in different income or demographic groups. (\cite{https://doi.org/10.3982/ECTA19815, https://doi.org/10.3982/ECTA19417}) Automation usually replaces low-skilled labor in the production process. In other words, the people who lose their jobs to AI are theoretically those who are already in the lower income bracket. Additionally, more automation means a higher return on capital investment, delivering more earnings to the wealthy. \cite{SKARE2024102719} shows that the capital stock of AI is unevenly distributed and positively correlated with exacerbated income inequality. Using online job posting data, \cite{ssrn4874061} finds that hiring for lower-level office jobs declined after ChatGPT's release, while demand for higher-skilled workers increased.

Economic inequality not only exists across income groups but also across demographics. Preliminary estimation in \cite{NBERw32487} suggests that AI might cause the wages of low-education women to decline. \cite{appelmccrorytamkin2025geoapi} finds that countries with higher AI usage are concentrated in North America, Western Europe, and Northeast Asia, while the global south almost entirely falls into the minimum usage group. In their theoretical framework, \cite{doi:10.1086/737233} found that AI is more likely to displace workers from complex problem-solving work to routine tasks in a developing economy, unlike in an advanced economy where people will be better positioned to use AI to help them tackle more complicated work. All of this seems to suggest that the prevalence of AI is empowering the already-empowered while underprivileged groups will benefit less from AI.

Rising economic inequality has significant implications. Long-lasting stagnant wages and loss of employment opportunities in certain groups create popular discontent that sometimes fuels disruptive populist movements. Uneven distribution of income also leads to the consolidation of wealth for certain individuals or business sectors, which in turn results in the consolidation of power. This is particularly true in the case of the current AI landscape, in which very few people possess immense power over the trajectory of its development and are able to steer the future of AI to benefit themselves. 

\subsection{Power Concentration}
{\itshape TODO: Add description on how the trajectory of AI is dictated by very few powerful entities/men. OpenAI's power struggle. Capital investment. Big techs are the only player. Effective Altruism}
\subsubsection{Death of Open Science}
The paradigm of foundation models represents the homogenization in the field of machine learning research and development. Specifically, \cite{bommasani2022opportunitiesrisksfoundationmodels} characterize homogenization as ``the consolidation of methodologies for building machine learning systems across a wide range of applications.'' The origin of foundation models stems from the field of Natural Language Processing (NLP), with key breakthroughs including self-supervised learning, word embedding, the transformer (\cite{vaswani2023attentionneed}), and the bi-directional encoder-decoder (\cite{devlin2019bertpretrainingdeepbidirectional}). These technical architectures represent the essential properties of foundation models, laying out their uncanny capabilities but also their intrinsic downfalls.

Today, almost all state-of-the-art NLP models are derived from a handful of foundation models like the bidirectional encoder model BERT. (\cite{bommasani2022opportunitiesrisksfoundationmodels}) The transformer architecture of large language models is also beginning to be widely applied to other areas like speech recognition, image, biology, and reinforcement learning. The phenomenon of foundation models as a paradigm taking over other forms of machine learning systems is the key motivation of this work, particularly because of the ongoing issues it's creating and the huge amount of resources required to develop them. AI, as a frontier discipline of science, should be explored openly. However, we are observing foundation models crowding out the resources for research and development in other machine learning systems. 

Even worse, the development of foundation models is so expensive that only a handful of ``big-techs'' can afford it now. It's getting more and more difficult for independent researchers unaffiliated with big-tech companies to conduct research in cutting-edge foundation models. Because of the sheer size of these models, their performance claimed by the private developers is extremely hard to verify independently. Evaluating their real-world cost and externalities has become equally difficult for the same reason.

Founded as a non-profit to ensure the safety of AI development, OpenAI stopped open-sourcing its models after GPT-3 in 2020, citing security reasons. This has become the norm across many big players in the field, including Google and Anthropic. (\cite{empire}) Subsequently, OpenAI has transformed itself into a private for-profit company. The field of cutting-edge foundation model development increasingly looks like another technology race in Silicon Valley, rather than an open community in which members share knowledge and breakthroughs with each other for the advancement of science that benefits humanity. If AI is what we believe to be the next leap forward in human civilization, the path we are taking to get there seems like an extremely dangerous one.

The thriving community of AI Ethics researchers set out to tackle these threats by collaborating on research evaluating the issues of foundation models and coming up with appropriate benchmarks. Nonetheless, big-techs still find their way to try to consolidate their power in this community. \cite{10.1145/3461702.3462563} finds that big-techs use the same strategy as big tobacco in the 1950s to undermine research questioning their products. They publicly claim their emphasis on and care for AI safety and even set up internal research divisions to conduct research. They also sponsor and provide funding for top conferences, events, and individuals in AI Ethics. This presents conflicts of interest and threats to academic integrity. According to the cited work, there are more researchers in AI Ethics at top institutions that have received funding from or been affiliated with big-techs than those who have not. In 2020, researcher Timnit Gebru was ousted by Google over the publication of her now-famous paper Stochastic Parrots, highlighting the issues of large language models sheerly repeating their own training data. (\cite{empire}, \cite{parrots})

\section{Forgotten paths}
Known as the ``father of AI'', congnitive and computer scientist Marvin Minsky wrote in his book \textit{The Society of Mind}, ``The power of intelligence stems from our vast diversity, not from any single, perfect principle.'' He portraits the human mind as a society of simple but very different cognitive processes, also known as agents. (\cite{societyofmind}) Each of these individual thinking entity has different internal logics, combined to form the vast intelligence of our human mind.

The current paradigm in Artificial Intelligence, however, is devoted to building systems that are the complete opposite. The paradigm of foundation model suggests that a single, humongous ``master algorithm'' will be able to reproduce the extremely complicated and inter-weaved cognitive capabilities of the human mind. In particular, the uprise of Large Language Models implies that language is the only necessary tool for intelligent systems to reason and make decisions. Instead of incorporating a fundamentally different architecture for vision capabilities, the recent development of multi-modal Vision-Language Models uses the same word embedding techniques for understanding images, essentially treating each small ``patch'' of an image as a word. (\cite{dosovitskiy2021imageworth16x16words}) It seems like the world firmly believes that this one single way of developing AI systems can lead to something on par with the human mind.

The core proposition of this thesis is that instead of pursuing an omni-capable master algorithm, resources should be more equally devoted to developing various kinds of AI systems. This section outlines the other alternative paradigms of AI that I believe should have been given the same amount of attention. Some of them draw inspiration from the intrinsic downfalls of foundation models, others from the negative externalities we are observing around the world, and still others from the cognitive science that depicts how human intelligence works.

\subsection{Application-Driven AI}
The premise of foundation models, or more specifically transformer-based large language models, as the major breakthrough towards artificial general intelligence is that one giant model will be sufficient to encapsulate all sorts of human intelligence. Indeed, in recent years, we've seen substantial improvement in the performance of these models. However, The so-called ``performance'' are usually evaluated with hypothetical datasets and metrics that doesn't translate to real-world capabilities.

This is what Kiri Wagstaff of the California Institute of Technology describes as ``machine learning for machine learning's sake''. (\cite{10.5555/3042573.3042809}) A lot of these improvements don't necessarily translate to real-world usages. For example, a computer vision model might be able to successfully detect pedestrians with a 99.9\% rate on a benchmark dataset; nevertheless, failing to identify one pedestrian in every 1000 is still fatal and doesn't mean the model is nearly close to being capable enough to be applied to critical applications like autonomous vehicles. The long tradition of machine learning is based on a stochastic process instead of deterministic decision-making. Much of artificial intelligence's vast improvement in performance is driven by raising the probability of success, not real-world usage. Moreover, the same metric for success rate is not comparable across different applications. 99.9\% might not be enough for self-driving cars but might be sufficient for identifying food ingredients. This is again to say that the pursuit of performance in benchmarking metrics tells us nothing about how usable it is in real-world applications.

Additionally, Wagstaff also argues that the machine learning community needs to engage with real-world domain experts more closely. The prevailing paradigm of collaboration with other domain experts usually only happens for curating and annotating training datasets (\cite{10.1145/3411764.3445518}), but little for creating the appropriate evaluations for real-world applications and for their potential impact.

Creating models that have real-world applications in mind from the initiation of their development also boasts the benefit of having smaller, more efficient models. Over the past couple of years, we've seen developments in machine vision, audio, and robotics all converge to the paradigm of a multi-modal transformer architecture. The one-model-to-rule-them-all concept does seem tempting, but the truth is that different tasks may require different architectures for more efficient computing. Even having more application-specific transformers may also be beneficial.

Smaller models also enable the possibility of being run locally on smaller hardware, which allows their users to have a very different relationship with the models. Models that can be run locally will also be easier to be trained and fine-tuned by their users, which means they can adapt more granularly to a user's personal needs and lifestyle.

\subsection{Cognitive Algorithm}\label{cognition}
If the goal of Artificial Intelligence is to build computing systems that rival human intelligence, it makes sense to mimic how the human mind works. Unfortunately, the human brain and mind are such complicated systems that to this date, we still haven't solved their mystery. However, psychology, cognitive and neural science are still able to give us some hints.

New York University professors Gary Marcus and Ernest Davis wrote in their book \textit{Rebooting AI} that drawing from Kant's \textit{Critique of Pure Reason}, ``time, space, and causality are the three philosophical grounds fundamental to the human mind.'' (\cite{reboot}) When seeing a car coming while trying to cross the road, humans are able, in almost no time, to estimate how far the car is, how long it's going to take for it to be dangerously near, and how long it would take to cross the road, and then make a decision. Such simple everyday tasks are usually comprised of many complicated thoughts. We are able to ``sense'' the distance and time of the car with little to no brain effort, at the same time considering other factors, like the sound volume of the coming car or whether you are about to be late or not.

\subsubsection{Purely Analog Senses}
Given a well-defined context and sufficient knowledge, existing machine learning systems might be able to understand the time duration of a certain event, but they are only able to do so by inducing from mathematical formulas. They have the knowledge of time, but humans have a sense of time. Similarly, AI has an understanding of the shapes of objects, but it might not be able to immediately induce their functionality from the shapes. Imagine seeing a cheese grater for the first time; we are able to reason with its scale, shape, material, and texture to immediately guess what it might be for and how to use it. This kind of holistic reasoning combining our innate understanding of time, space, and causality is exactly the kind that intelligent machines should learn from the human mind. Of course, by showing a cheese grater and its text description to a model during training, current machine learning systems would have no problem understanding what a cheese grater is. But perhaps that's why foundation models have to be large models consuming almost all the available existing data to train. It is important to note that fundamentally, current state-of-the-art models are still language models that understand everything through language and language only. Computers fundamentally cannot ``sense'' anything but can understand its ``abstraction'' as language or other symbolic systems, like a mathematical formula.

A maybe more efficient way to build AI systems is to look into the other ``senses'' that do not yet exist in the digital world so that AI can reason and induce more holistically. Upon seeing the picture of a textile, we are immediately able to guess its texture, flexibility, or even whether it's waterproof or not, all without touching it. The existing ``senses'' computers have include light, sound, force, temperature, electrical potential, magnetic fields, and more. All of these are mathematically well-defined physical properties, and they all have corresponding sensors that can transform them into digital readouts. We humans, however, do not rely on the understanding of physical properties to perceive the world. We understand the world through smell, texture, haptics, and more. Machine learning systems, on the other hand, do not possess this kind of capability to understand these ``mathematically ill-defined'' and thus purely analog properties. 

Some recent developments in the field of the so-called world model claim to be able to generate realistic simulations of the physical world. (\cite{bruce2024geniegenerativeinteractiveenvironments,gupta2022maskvitmaskedvisualpretraining}) What these models actually are is models capable of playing video games. It is unsure whether they are just reproducing common physical phenomena in video training data or if they indeed have an understanding of the rules of physics. Even if they do, their output is still limited to just video or visual images -- far from all the complicated analog senses humans have. We should tap into these unknown territories of the digital world, trying to come up with more abstraction or symbolic systems to teach machines more nuances of this physical world. This can potentially eliminate the vast resources required to teach models everything with empirical training materials. And in the context of the current paradigm of large language foundation models, giving computing devices more ``senses'' to characterize the world also saves us the effort and data size of trying to describe everything in text.  

\subsubsection{Common Senses and Causality}
When we say Novak Djokovic has been playing professional tennis for 20 years, we don't mean that he has been playing tennis non-stop for the continuous time duration of 20 years. The human mind is capable of understanding the implied meaning that tennis has been Novak Djokovic's profession for 20 years. However, a language model that is trained to understand the literal meaning of texts might not be able to decipher the latent implication.\footnote{I prompted Claude Sonnet 4.5, ``suppose one year is 365 days, one day is 24 hours, one hour is 60 minutes. Novak Djokovic has been playing tennis for 20 years, how long has he been playing tennis?'', and then the chatbot went on to actually convert 20 years to hours and minutes. Although it is also quite astonishing that the model was able to understand the implied meaning of the first part of the prompt, a human being would probably first ask for clarification.} This is an example of what we vaguely describe as ``common sense''. We, as humans, do not need to be taught that it is impossible to continuously play tennis for 20 years, even for a professional athlete, and are able to comprehend the idea of a profession because we are able to understand the world and human lives by employing different knowledge frameworks simultaneously. Instead of reading 20 years as 365 days and one day as 24 hours, our sense of 20 years as ``a very long time'' substantially outweighs the symbolic meaning of a year as a unit of time. The human mind is firstly a sensing entity and then an abstraction decoder. We built the various symbolic systems based on our innate senses; machine learning systems, however, can only induce from the abstraction and symbols. As a result, the lack of common sense has been a challenge for AI.

An alternative AI paradigm is trying to program common sense, or the general understanding of causality, into purely mathematical models. We understand the basic causal relationship of everyday events and objects, but even the largest foundation model today does not have any mechanism to employ this kind of knowledge. It only has the most surface-level understanding of our common sense even after being trained on the largest corpus of text material. Carnegie Mellon University computer scientist Douglas Lenat was an advocate for intelligent machine systems with programmed causality. (\cite{LENAT1984269}) His long-term project Cyc aims to create a symbolic system and a knowledge base database that maps out the relationship of everyday common sense and implicit knowledge. It's a decision-making algorithm that is based on purely deterministic and pre-programmed relationships. After almost 40 years of employing philosophers and programmers to write down rules of the world for machines to understand, the project unfortunately never gained much traction. However, it is a sharp contrast to the strictly stochastic and training-based paradigm of foundation models. 

Amidst the enormous resources being poured into developing large models, maybe it is a good time to revisit the idea that machines should be able to employ some simple pre-programmed rules that describe how the world works, instead of trying to make the algorithms learn every single relationship embedded in our common sense. 

\subsubsection{Learning versus Innateness}
Large foundation models, as statistical pattern-matching machines, try to generalize everything into a widely applicable algorithm. Even if they were able to learn the entire common sense of the human mind from a large corpus of data, not everything follows rules. Our human intelligence makes decisions based on not only the abstraction and generalization of our various senses and experiences but also our innate values. We internalize our experiences into an individual's unique logic.

Psychology provides us with plenty of evidence that human decision-making does not only come from learnings from the outside world but also from internalized values. Psychologist John Watson of Johns Hopkins University, a major pioneer in \textit{behaviorism} psychology in the early 1900s, famously claimed that a child's behavioral pattern can be dictated purely by controlling every factor of the environment in which the child was raised. (\cite{behaviorism}) Only a couple of decades later, Noam Chomsky, who is usually seen as the founder of \textit{cognitive} psychology, published the groundbreaking work in linguistics \textit{Syntactic Structures}. (\cite{noamchomskylinguistic}). Along with his other works, he characterizes language understanding not as pattern-matching of past encounters of sentences but as a result of internalized grammar. (\cite{597748ff-2020-3c20-9c06-e358280e09a2}) The old perspective of behaviorism explains behavioral patterns entirely with an external reward system; in contrast, Noam Chomsky's cognitive psychology attributes the driving force of human intelligence to internal representations like desires, beliefs, and purpose. To this date, this view from cognitive psychology is still agreed upon by scholars, while behaviorism theories have vanished. (\cite{reboot})

The paradigm of foundation models, in a sense, is like dictating a child's behavior by raising them in a controlled environment. Every model starts from a bunch of random numbers and then learns everything from data by maximizing rewards or minimizing loss. Although so far we've seen this approach make big progress, it is an extremely resource-intensive process, and we must remember that this is not how the human mind works. We should look into ways to design models that are capable of internalizing values and generating internal representations of these values. Structuring experiences and learnings to create a unique internal logic is a crucial ``human'' aspect of human intelligence. Artificial intelligence should also be able to replicate its user's internal representation in the training/inferencing process. Most large language models might be able to adapt to a user's response, but they do so by appending the user's previous feedback to future prompts. Internally, it's just the same model processing longer inputs with more added context. Compared to the human mind's capability of generating internal representations, this is extremely inefficient and again adds to the already enormous resources required to run these models.
\subsection{Trust}
{\itshape TODO: Talk about why people lose their trust on AI? How can we rebuild trust? Can we reimagine our relationship with AI? Perhaps more private, transparent and tactile experience.}

\subsection{Participatory AI}
{\itshape TODO: A more democratized process of AI development. Data-centric AI (\cite{Liang2022,10.1145/3411764.3445518})}

\section{Observations}
\subsection{AI is not good enough}
By saying that AI would benefit the world because it can solve cancer, climate change and poverty, popular AI advocative sentiment usually implies that if we solve AI, then we essentially solve everything. However, the actual implication of a solved world is far beyond general prosperity and panacae. Philosopher Nick Bostrom describes the condition of a solved world as \textit{Technology Maturity}, meaning that with AI we would have all the technologies we can possibly have and use them to solve whatever problem that remains. (\cite{bostromdeeputopia}) When we look into what exactly these technology are, you will see that we are still so far away from technology maturity.

{\itshape TODO: Examples. Brain-computer interface, solving AI is almost contingent on solving the human brain, which we are still far. }
\subsubsection{Language alone is not enough}
\subsection{Is AI ever going to be good enough?}
{\itshape TODO: Can we have human-level intelligence machine without inducing moral-status? Sentience machine (\cite{bostromdeeputopia})}
\subsubsection{Do we really want AI to be that good?}
{\itshape TODO: Purpose? Return of Malthusian economy. Extreme inequality -- all earning goes to capital. Reference Sec. \ref{econ}}
\subsection{AI is machine, not human}
The discussion of the long-term utopian (or dystopian) vision and speculation of AI is beyond the scope of this design work. We are designing for the world with current technologies and socioeconomical conditions. However, extrapolating the extreme future informs us a lot about what the currrent vesion of AI we have actually is and how it sits in the imaginable roadmap of the technology. Only once we have those understandings can we design for the AI we have instead the AI we envision.

Following previous discussions, we know that the current version of AI we have is still far away from what's promised. That is, the technology itself is still not good enough. However, in the discipline of design, we are designing the derivative product or human interface of AI \textit{as if} the technology was that good. The perimeter of the very limited capability of models is intentionally invisible. The intrinsic downfalls and externalities are hidden. In pursuing ease-of-use and ``seamless user experience'', user-configurablity is completely abandoned. It seems that designers want users to believe that the models can figure out what exactly what's needed where in fact more means of user input could potentially improve the performance dramatically. The prevalent chatbot interface gives up on informing the user how models work, essentially depriving user's oppurtunity to make models work better for them.

Chatbot is a human-human interface that lives on computing devices, not a human-computer interface. One of the implication of technology maturity is that whatever intelligence machine we come up with, we can interact with it in a manner extremely similar to human-human interaction. AI, as it is now, is a sort of intelligence machine that is still far from technology maturity. However, that interface that we came up with is the one that is supposed to make sense only once technology maturity is reached. In designing a interface like chatbot, the desginer forgot the fact that AI is still fundamentally a machine. Machines need to be operated in a certain way to yield maximum performance and efficiency. Operating a machine requires that the user has some technical knowledge. A well-planned learning curve is what makes a human interface usable. Typing on a keyboard is a learning curve. Scrolling, pinch-to-zoom, long-press on a touch screen all demand leanring. For a technology like AI that is still novel to the public, opting for a human-like interface that has close to no learning curve seems more like evading the hard work rather than being considerate.

In every other technology humanity has ever seen before, its human interface is always one that bridges the needs of users and the limitations of the technology. The purpose of human interface is to make the technology works for the human, hence the interface has to work for both the technology and the human. Chatbot, however, is one that only works for human but takes no consideration for the intrinsic properties and operational parameters of the models.

A good human interface informs the user the capabilities (and limitations) of the technology, best operational practices and potential means of trouble-shooting. Every usable machine in the history has a human interface that accomplishes these things. When a machinist is operating a laith, they can observe whether the power, gear ratio and cutting tool are making a clean cut. If they want to change the speed of the rotor, it's clear that the most preferable way is to change the gear ratio. If the cut is not clean even under optimal rotational speed, they would know maybe the cutting tool needs to be sharpened. Using a laith requires a somewhat steep learning curve, but at least there is one. On a desktop computer interface, the relative size of the menu bar vaguely indicates the capacity for multitasking. You can easily drag and resize each window to have the best combination of applications running. If the computer becomes slow and laggy, most people can easily induce that they should close some windows to keep everything running smoothly. These are all examples of how a human interface bridges the gap between what the user wants to do and what the technology can do. 

What the chatbot interface assumes is that the user can do all of these by giving more instructions in natural language to correct or optimize the model's performance. If the AI technology we have is good enough in terms of its cognitive capabalities (see Section \ref{cognition}), this is a quite viable assumption. However, our AI is still not there and this chatbot interface only results in prompts augmented with more and longer prompts, which ultimately exceeds the context window of the model and makes it disoriented. Chatbot is an interface for the AI that can do everything (or, one that is very close to reach technology maturity), but it is an extremely ineffective and inefficient way to ``operate'' today's large language models.

{\itshape TODO: Why augmenting chatbot instead of radical new design. Because it's langague model. Because language is the primary way of communication}
\printbibliography[heading=bibintoc]
\end{document}